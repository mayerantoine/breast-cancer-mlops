{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Breast Cancer - Optimizing an Azure ML Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1662051957855
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1662051959188
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Azure ML SDK Version:  1.43.0\n",
            "1.1.2\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import azureml\n",
        "import os\n",
        "import sklearn\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from azureml.core import Run, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from azureml.core import Workspace, Experiment, Run\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "from azureml.core import ScriptRunConfig, Environment\n",
        "from azureml.widgets import RunDetails\n",
        "from azureml.data.data_reference import DataReference\n",
        "from azureml.pipeline.core import Pipeline, PipelineData\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "\n",
        "# check core SDK version number\n",
        "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
        "print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1662051983348
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import  Workspace\n",
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"9ce70869-60db-44fd-abe8-d2767077fc8f\")\n",
        "\n",
        "ws = Workspace.from_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1662051983551
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Workspace name: cdh-azml-dev-mlw\n",
            "Azure region: eastus\n",
            "Subscription id: 320d8d57-c87c-4434-827f-59ee7d86687a\n",
            "Resource group: csels-cdh-dev\n"
          ]
        }
      ],
      "source": [
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1662051990075
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Find the existing cluster\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "source": [
        "clustername = 'StandardDS12CPU'\n",
        "is_new_cluster = False\n",
        "try:\n",
        "    aml_compute = ComputeTarget(workspace = ws,name= clustername)\n",
        "    print(\"Find the existing cluster\")\n",
        "except ComputeTargetException:\n",
        "    print(\"Cluster not find - Creating cluster.....\")\n",
        "    is_new_cluster = True\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
        "                                                            vnet_name='csels-cdh-dev-vnet',\n",
        "                                                            vnet_resourcegroup_name='CSELS-CDH-DEV',\n",
        "                                                            subnet_name='cdh-azml-dev-snet',\n",
        "                                                           max_nodes=4)\n",
        "    aml_compute = ComputeTarget.create(ws, clustername, compute_config)\n",
        "\n",
        "aml_compute.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1662052022993
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\"Datastore.upload\" is deprecated after version 1.0.69. Please use \"Dataset.File.upload_directory\" to upload your files             from a local directory and create FileDataset in single method call. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading an estimated of 9 files\n",
            "Uploading ./data/.amlignore\n",
            "Uploaded ./data/.amlignore, 1 files out of an estimated total of 9\n",
            "Uploading ./data/.amlignore.amltmp\n",
            "Uploaded ./data/.amlignore.amltmp, 2 files out of an estimated total of 9\n",
            "Uploading ./data/cancer_data.csv\n",
            "Uploaded ./data/cancer_data.csv, 3 files out of an estimated total of 9\n",
            "Uploading ./data/test/.amlignore\n",
            "Uploaded ./data/test/.amlignore, 4 files out of an estimated total of 9\n",
            "Uploading ./data/test/.amlignore.amltmp\n",
            "Uploaded ./data/test/.amlignore.amltmp, 5 files out of an estimated total of 9\n",
            "Uploading ./data/test/test.csv\n",
            "Uploaded ./data/test/test.csv, 6 files out of an estimated total of 9\n",
            "Uploading ./data/train/.amlignore\n",
            "Uploaded ./data/train/.amlignore, 7 files out of an estimated total of 9\n",
            "Uploading ./data/train/.amlignore.amltmp\n",
            "Uploaded ./data/train/.amlignore.amltmp, 8 files out of an estimated total of 9\n",
            "Uploading ./data/train/train.csv\n",
            "Uploaded ./data/train/train.csv, 9 files out of an estimated total of 9\n",
            "Uploaded 9 files\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{\n",
              "  \"source\": [\n",
              "    \"('workspaceblobstore', 'cancer_data/cancer_data.csv')\"\n",
              "  ],\n",
              "  \"definition\": [\n",
              "    \"GetDatastoreFiles\",\n",
              "    \"ParseDelimited\",\n",
              "    \"DropColumns\",\n",
              "    \"SetColumnTypes\"\n",
              "  ],\n",
              "  \"registration\": {\n",
              "    \"id\": \"00e927e8-232d-4359-9008-587df5ceaa1b\",\n",
              "    \"name\": \"raw_data\",\n",
              "    \"version\": 1,\n",
              "    \"workspace\": \"Workspace.create(name='cdh-azml-dev-mlw', subscription_id='320d8d57-c87c-4434-827f-59ee7d86687a', resource_group='csels-cdh-dev')\"\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_store = ws.get_default_datastore()\n",
        "data_store.upload(src_dir='./data',target_path='cancer_data',overwrite=True,show_progress=True)\n",
        "ds_raw = Dataset.Tabular.from_delimited_files(path=data_store.path('cancer_data/cancer_data.csv'))\n",
        "ds_raw.register(workspace=ws,name='raw_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1662052023274
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ds_raw = ds_raw.as_named_input('raw_data')\n",
        "\n",
        "\n",
        "train_data = PipelineData(\"train_cancer_data\",datastore=data_store).as_dataset()\n",
        "test_data = PipelineData(\"test_cancer_data\",datastore=data_store).as_dataset()\n",
        "model_file = PipelineData(\"model_file\",datastore=data_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1662052023465
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Use a RunConfiguration to specify some additional requirements for this step.\n",
        "from azureml.core.runconfig import RunConfiguration,DockerConfiguration\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "\n",
        "# create a new runconfig object\n",
        "run_config = RunConfiguration()\n",
        "\n",
        "# set Docker base image to the default CPU-based image\n",
        "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
        "\n",
        "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
        "run_config.environment.python.user_managed_dependencies = False\n",
        "\n",
        "# specify CondaDependencies obj\n",
        "run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
        "    conda_packages=['pandas','numpy'],\n",
        "    pip_packages=['scikit-learn','joblib','azureml-sdk'],\n",
        "    pin_sdk_version=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1661456432345
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./scripts/prepare.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./scripts/prepare.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sklearn\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from azureml.core import  Workspace\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from azureml.core import Run, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser(\"prepare\")\n",
        "\n",
        "    parser.add_argument(\"--input_data\",type=str)\n",
        "    parser.add_argument(\"--train\",type=str)\n",
        "    parser.add_argument(\"--test\",type=str)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"train args:\",args.train)\n",
        "    \n",
        "    run = Run.get_context()\n",
        "    ws = run.experiment.workspace\n",
        "    ds_tr = ws.get_default_datastore()\n",
        "\n",
        "\n",
        "    df = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
        "\n",
        "    y = df['diagnosis'].astype('category')\n",
        "    X = df.drop('diagnosis',axis=1)\n",
        "\n",
        "    lbl_encoder = LabelEncoder()\n",
        "    y_encode = lbl_encoder.fit_transform(y)\n",
        "\n",
        "    print(\"cols:\",X.columns)\n",
        "    print(\"X shape\", X.shape)\n",
        "    print(\"encoder:\", lbl_encoder.classes_)\n",
        "    print(\"y encode:\", y_encode.shape)\n",
        "\n",
        "    x_train,x_test,y_train,y_test = train_test_split(X,y_encode,train_size=0.75,random_state=42,stratify =y_encode)\n",
        "\n",
        "    print(x_train.shape)\n",
        "    print(y_train.shape)\n",
        "\n",
        "    print(x_test.shape)\n",
        "    print(y_test.shape)\n",
        "\n",
        "    train = np.column_stack([x_train,y_train])\n",
        "    test = np.column_stack([x_test,y_test])\n",
        "   \n",
        "    # Write the model to file.\n",
        "    train_path = \"./data/train/\"\n",
        "    test_path = \"./data/test/\"\n",
        "\n",
        "    os.makedirs(args.train, exist_ok=True)\n",
        "    os.makedirs(args.test, exist_ok=True)\n",
        "    print(\"Saving the split\")\n",
        "\n",
        "    np.savetxt(os.path.join(args.train,\"train.csv\"), train, delimiter=\",\")\n",
        "    np.savetxt(os.path.join(args.test,\"test.csv\"), train, delimiter=\",\")\n",
        "  \n",
        "\n",
        " \n",
        "if __name__ =='__main__':\n",
        "    main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./scripts/train2.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./scripts/train2.py\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sklearn\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from azureml.core import Run, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\"train\")\n",
        "    \n",
        "    parser.add_argument(\"--train\", type=str, help=\"train data\")\n",
        "    parser.add_argument(\"--test\", type=str, help=\"test data\")\n",
        "    parser.add_argument(\"--model_file\", type=str, help=\"model file\")\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    run = Run.get_context()\n",
        "    ws = run.experiment.workspace\n",
        "    ds_tr = ws.get_default_datastore()\n",
        "\n",
        "    print(args.train)\n",
        "    print(args.test)\n",
        "\n",
        "    train = pd.read_csv(args.train+\"/train.csv\")\n",
        "    test = pd.read_csv(args.test+\"/test.csv\")\n",
        "\n",
        "    y_train = train.iloc[:,-1]\n",
        "    train.drop(columns = train.columns[-1],axis=1,inplace=True)\n",
        "    x_train = train\n",
        "\n",
        "    y_test = test.iloc[:,-1]\n",
        "    test.drop(columns = test.columns[-1],axis=1,inplace=True)\n",
        "    x_test = test\n",
        "\n",
        "    lbl_encoder = LabelEncoder()\n",
        "    y_encode = lbl_encoder.fit_transform(y_train)\n",
        "\n",
        "    print(\"cols:\",x_train.columns)\n",
        "    print(\"X shape\", x_train.shape)\n",
        "    print(\"encoder:\", lbl_encoder.classes_)\n",
        "    print(\"y encode:\", y_encode.shape)\n",
        "\n",
        "\n",
        "    print(x_train.shape)\n",
        "    print(y_train.shape)\n",
        "\n",
        "    print(x_test.shape)\n",
        "    print(y_test.shape)\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=40,max_depth=100,max_features=None,min_samples_leaf=3)\n",
        "    rf.fit(x_train,y_train)\n",
        "\n",
        "    accuracy = accuracy_score(y_test,rf.predict(x_test))\n",
        "    run.log(\"accuracy\",accuracy)\n",
        "\n",
        "    f1 = f1_score(y_test,rf.predict(x_test))\n",
        "    run.log(\"f1_score\",f1)\n",
        "\n",
        "\n",
        "    # Write the model to file.\n",
        "    # model_path = \"./outputs/cancer_model.pkl\"\n",
        "    os.makedirs(args.model_file, exist_ok=True)\n",
        "    joblib.dump(rf, args.model_file+\"/cancer_model.pkl\")\n",
        "\n",
        "    print('Saving the model to {}'.format(args.model_file+\"/cancer_model.pkl\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    run.complete()\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1662052024009
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step1 created\n"
          ]
        }
      ],
      "source": [
        "source_directory ='./scripts'\n",
        "step1 = PythonScriptStep(name=\"prepare_step\",\n",
        "                         script_name=\"prepare.py\", \n",
        "                         arguments=[\"--input_data\",ds_raw,\"--train\",train_data,\"--test\",test_data],\n",
        "                         inputs=[ds_raw],\n",
        "                         outputs=[train_data,test_data],\n",
        "                         compute_target=aml_compute, \n",
        "                         runconfig=run_config,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "print(\"Step1 created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1662052024287
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step2 created\n"
          ]
        }
      ],
      "source": [
        "step2 = PythonScriptStep(name=\"train_step\",\n",
        "                         script_name=\"train2.py\", \n",
        "                         arguments=[\"--train\",train_data,\"--test\",test_data,\"--model_file\",model_file],\n",
        "                         inputs=[train_data,test_data],\n",
        "                         outputs=[model_file],\n",
        "                         compute_target=aml_compute, \n",
        "                         runconfig=run_config,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "print(\"Step2 created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1662052024553
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.1.2'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sklearn\n",
        "sklearn.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./scripts/register.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./scripts/register.py\n",
        "import argparse\n",
        "import sklearn\n",
        "from azureml.core import Run, Dataset\n",
        "from azureml.core.model import Model as AMLModel\n",
        "from azureml.core.resource_configuration import ResourceConfiguration\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "    parser = argparse.ArgumentParser(\"register\")\n",
        "    \n",
        "    parser.add_argument(\"--model_file\", type=str, help=\"model file\")\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    run = Run.get_context()\n",
        "    ws = run.experiment.workspace\n",
        "    ds_tr = ws.get_default_datastore()\n",
        "\n",
        "    model_path = args.model_file+\"/cancer_model.pkl\"\n",
        "\n",
        "    print(\"model path:\",model_path)\n",
        "\n",
        "    AMLModel.register(workspace=ws,\n",
        "                      model_name=\"breast-cancer\",\n",
        "                      model_path=model_path,\n",
        "                      model_framework=AMLModel.Framework.SCIKITLEARN,\n",
        "                      model_framework_version=sklearn.__version__,\n",
        "                      resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n",
        "                      description='Random forest classification model to predict breast cancer',\n",
        "                       tags={'area': 'cancer', 'type': 'classification'})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1662052024917
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step3 created\n"
          ]
        }
      ],
      "source": [
        "step3 = PythonScriptStep(name=\"register_step\",\n",
        "                         script_name=\"register.py\", \n",
        "                         arguments=[\"--model_file\",model_file],\n",
        "                         inputs=[model_file],\n",
        "                         compute_target=aml_compute, \n",
        "                         runconfig=run_config,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "print(\"Step3 created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1662052028920
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "steps = [step1,step2,step3]\n",
        "pipeline1 = Pipeline(workspace=ws,steps=steps,default_datastore=data_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1662052035114
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "run_exp = Experiment(workspace=ws, name=\"RF-BreastCancer-Pipeline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1662052064130
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created step prepare_step [a88bfcb4][10215e50-7deb-4977-9c62-a792844ecfeb], (This step will run and generate new outputs)\n",
            "Created step train_step [1eb6af8e][36c28b7f-2a5d-4f8b-95fc-81612ba78c82], (This step will run and generate new outputs)\n",
            "Created step register_step [c4231b9c][f5f43ca0-2f45-43e1-b071-58ee724af485], (This step will run and generate new outputs)\n",
            "Submitted PipelineRun dbdce8c6-3fbf-4fc5-8544-d001c3dfe40f\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/dbdce8c6-3fbf-4fc5-8544-d001c3dfe40f?wsid=/subscriptions/320d8d57-c87c-4434-827f-59ee7d86687a/resourcegroups/csels-cdh-dev/workspaces/cdh-azml-dev-mlw&tid=9ce70869-60db-44fd-abe8-d2767077fc8f\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>RF-BreastCancer-Pipeline</td><td>dbdce8c6-3fbf-4fc5-8544-d001c3dfe40f</td><td>azureml.PipelineRun</td><td>Running</td><td><a href=\"https://ml.azure.com/runs/dbdce8c6-3fbf-4fc5-8544-d001c3dfe40f?wsid=/subscriptions/320d8d57-c87c-4434-827f-59ee7d86687a/resourcegroups/csels-cdh-dev/workspaces/cdh-azml-dev-mlw&amp;tid=9ce70869-60db-44fd-abe8-d2767077fc8f\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
            ],
            "text/plain": [
              "Run(Experiment: RF-BreastCancer-Pipeline,\n",
              "Id: dbdce8c6-3fbf-4fc5-8544-d001c3dfe40f,\n",
              "Type: azureml.PipelineRun,\n",
              "Status: Running)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_exp.submit(pipeline1,regenerate_ouputs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1662052064277
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/batch/tasks/shared/LS_root/mounts/clusters/wsn8-su2/code/Users/WSN8-SU\n"
          ]
        }
      ],
      "source": [
        "os.getcwd()\n",
        "print(os.path.dirname(os.getcwd()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./scripts/build_train_pipeline.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./scripts/build_train_pipeline.py\n",
        "\n",
        "import azureml\n",
        "import os\n",
        "import sklearn\n",
        "from azureml.core import  Workspace\n",
        "from azureml.pipeline.core.graph import PipelineParameter\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.pipeline.core import Pipeline, PipelineData\n",
        "from azureml.core import Workspace, Dataset, Datastore\n",
        "from azureml.core.runconfig import RunConfiguration,DockerConfiguration\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core import Workspace, Experiment, Run\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "from azureml.core import ScriptRunConfig, Environment\n",
        "from azureml.widgets import RunDetails\n",
        "from azureml.data.data_reference import DataReference\n",
        "from azureml.pipeline.core import Pipeline, PipelineData\n",
        "\n",
        "\n",
        "def get_aml_compute(workspace):\n",
        "    clustername = 'StandardDS12CPU'\n",
        "    is_new_cluster = False\n",
        "    try:\n",
        "        aml_compute = ComputeTarget(workspace = workspace,name= clustername)\n",
        "        print(\"Find the existing cluster\")\n",
        "    except ComputeTargetException:\n",
        "        print(\"Cluster not find - Creating cluster.....\")\n",
        "        is_new_cluster = True\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
        "                                                                vnet_name='csels-cdh-dev-vnet',\n",
        "                                                                vnet_resourcegroup_name='CSELS-CDH-DEV',\n",
        "                                                                subnet_name='cdh-azml-dev-snet',\n",
        "                                                            max_nodes=4)\n",
        "        aml_compute = ComputeTarget.create(workspace, clustername, compute_config)\n",
        "\n",
        "    aml_compute.wait_for_completion(show_output=True)\n",
        "        \n",
        "    return aml_compute\n",
        "\n",
        "\n",
        "def get_environment():\n",
        "    # create a new runconfig object\n",
        "    run_config = RunConfiguration()\n",
        "\n",
        "    # set Docker base image to the default CPU-based image\n",
        "    run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
        "\n",
        "    # use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
        "    run_config.environment.python.user_managed_dependencies = False\n",
        "\n",
        "    # specify CondaDependencies obj\n",
        "    run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
        "        conda_packages=['pandas','numpy'],\n",
        "        pip_packages=['scikit-learn','joblib','azureml-sdk'],\n",
        "        pin_sdk_version=False)\n",
        "\n",
        "    return run_config\n",
        "\n",
        "def main():\n",
        "    \n",
        "    print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
        "    print(sklearn.__version__)\n",
        "\n",
        "    # Get workspace \n",
        "\n",
        "    ws = Workspace.from_config()\n",
        "    print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
        "\n",
        "\n",
        "    # Create AML compute\n",
        "    aml_compute = get_aml_compute(workspace=ws)\n",
        "\n",
        "    # Load and register the raw data \n",
        "    ## TODO We will need to change this as a parameter for new data\n",
        "    cur_dir = os.getcwd()\n",
        "    parent_dir = os.path.dirname(cur_dir)\n",
        "    src_dir = os.path.join(cur_dir,'data')\n",
        "\n",
        "    print(src_dir)\n",
        "\n",
        "    data_store = ws.get_default_datastore()\n",
        "    data_store.upload(src_dir=src_dir,target_path='cancer_data',overwrite=True,show_progress=True)\n",
        "    \n",
        "    ds_raw = Dataset.Tabular.from_delimited_files(path=data_store.path('cancer_data/cancer_data.csv'))\n",
        "    ds_raw.register(workspace=ws,name='raw_data')\n",
        "\n",
        "    # Create PipelineData\n",
        "\n",
        "    ds_raw = ds_raw.as_named_input('raw_data')\n",
        "    train_data = PipelineData(\"train_cancer_data\",datastore=data_store).as_dataset()\n",
        "    test_data = PipelineData(\"test_cancer_data\",datastore=data_store).as_dataset()\n",
        "    model_file = PipelineData(\"model_file\",datastore=data_store)\n",
        "\n",
        "\n",
        "    # Create Python Envirronment \n",
        "    run_config = get_environment()\n",
        "\n",
        "\n",
        "    # \n",
        "    source_directory ='./scripts'\n",
        "    step1 = PythonScriptStep(name=\"prepare_step\",\n",
        "                            script_name=\"prepare.py\", \n",
        "                            arguments=[\"--input_data\",ds_raw,\"--train\",train_data,\"--test\",test_data],\n",
        "                            inputs=[ds_raw],\n",
        "                            outputs=[train_data,test_data],\n",
        "                            compute_target=aml_compute, \n",
        "                            runconfig=run_config,\n",
        "                            source_directory=source_directory,\n",
        "                            allow_reuse=True)\n",
        "    print(\"Step Prepare created\")\n",
        "\n",
        "    step2 = PythonScriptStep(name=\"train_step\",\n",
        "                         script_name=\"train_step.py\", \n",
        "                         arguments=[\"--train\",train_data,\"--test\",test_data,\"--model_file\",model_file],\n",
        "                         inputs=[train_data,test_data],\n",
        "                         outputs=[model_file],\n",
        "                         compute_target=aml_compute, \n",
        "                         runconfig=run_config,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "    print(\"Step Train created\")\n",
        "\n",
        "    step3 = PythonScriptStep(name=\"register_step\",\n",
        "                         script_name=\"register.py\", \n",
        "                         arguments=[\"--model_file\",model_file],\n",
        "                         inputs=[model_file],\n",
        "                         compute_target=aml_compute, \n",
        "                         runconfig=run_config,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "    print(\"Step Register created\")\n",
        "\n",
        "    steps = [step1,step2,step3]\n",
        "    pipeline1 = Pipeline(workspace=ws,steps=steps)\n",
        "    \n",
        "    run_exp = Experiment(workspace=ws, name=\"RF-BreastCancer-Pipeline\")\n",
        "\n",
        "    run_exp.submit(pipeline1,regenerate_ouputs=False)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Azure ML SDK Version:  1.43.0\n",
            "1.1.2\n",
            "Workspace name: cdh-azml-dev-mlw\n",
            "Azure region: eastus\n",
            "Subscription id: 320d8d57-c87c-4434-827f-59ee7d86687a\n",
            "Resource group: csels-cdh-dev\n",
            "Traceback (most recent call last):\n",
            "  File \"./scripts/build_train_pipeline.py\", line 148, in <module>\n",
            "    main()\n",
            "  File \"./scripts/build_train_pipeline.py\", line 76, in main\n",
            "    aml_compute = get_aml_compute(workspace=ws)\n",
            "  File \"./scripts/build_train_pipeline.py\", line 26, in get_aml_compute\n",
            "    aml_compute = ComputeTarget(workspace = ws,name= clustername)\n",
            "NameError: name 'ws' is not defined\n"
          ]
        }
      ],
      "source": [
        "!python ./scripts/build_train_pipeline.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 ('azureml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "c29d249f6248e1876db3a43ab8bde2e53ec2cf908dd5eb31493a2d1f2322e9b6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
