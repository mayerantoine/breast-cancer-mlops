{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Breast Cancer - Optimizing an Azure ML Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\r\n",
        "import azureml\r\n",
        "import os\r\n",
        "import sklearn\r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "from sklearn.metrics import f1_score,accuracy_score\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from azureml.core import Run, Dataset\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from azureml.core import Workspace, Experiment, Run\r\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "from azureml.core import ScriptRunConfig, Environment\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "from azureml.data.data_reference import DataReference\r\n",
        "from azureml.pipeline.core import Pipeline, PipelineData\r\n",
        "from azureml.pipeline.steps import PythonScriptStep\r\n",
        "\r\n",
        "# check core SDK version number\r\n",
        "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Azure ML SDK Version:  1.43.0\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1661865907144
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import  Workspace\r\n",
        "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
        "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"9ce70869-60db-44fd-abe8-d2767077fc8f\")\r\n",
        "\r\n",
        "ws = Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1661865907808
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Workspace name: ' + ws.name, \r\n",
        "      'Azure region: ' + ws.location, \r\n",
        "      'Subscription id: ' + ws.subscription_id, \r\n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Workspace name: cselscdhazureml\nAzure region: eastus2\nSubscription id: 320d8d57-c87c-4434-827f-59ee7d86687a\nResource group: csels-cdh-dev\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1661865907939
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clustername = 'StandardDS12CPU'\r\n",
        "is_new_cluster = False\r\n",
        "try:\r\n",
        "    aml_compute = ComputeTarget(workspace = ws,name= clustername)\r\n",
        "    print(\"Find the existing cluster\")\r\n",
        "except ComputeTargetException:\r\n",
        "    print(\"Cluster not find - Creating cluster\")\r\n",
        "    is_new_cluster = True\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\r\n",
        "                                                           max_nodes=4)\r\n",
        "    aml_compute = ComputeTarget.create(ws, clustername, compute_config)\r\n",
        "\r\n",
        "aml_compute.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Find the existing cluster\nSucceeded\nAmlCompute wait for completion finished\n\nMinimum number of nodes requested have been provisioned\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1661865908820
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_store = ws.get_default_datastore()\r\n",
        "data_store.upload(src_dir='./data',target_path='cancer_data',overwrite=True,show_progress=True)\r\n",
        "ds_raw = Dataset.Tabular.from_delimited_files(path=data_store.path('cancer_data/cancer_data.csv'))\r\n",
        "ds_raw.register(workspace=ws,name='raw_data')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\"Datastore.upload\" is deprecated after version 1.0.69. Please use \"Dataset.File.upload_directory\" to upload your files             from a local directory and create FileDataset in single method call. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Uploading an estimated of 5 files\nUploading ./data/.amlignore\nUploaded ./data/.amlignore, 1 files out of an estimated total of 5\nUploading ./data/.amlignore.amltmp\nUploaded ./data/.amlignore.amltmp, 2 files out of an estimated total of 5\nUploading ./data/cancer_data.csv\nUploaded ./data/cancer_data.csv, 3 files out of an estimated total of 5\nUploading ./data/test/test.csv\nUploaded ./data/test/test.csv, 4 files out of an estimated total of 5\nUploading ./data/train/train.csv\nUploaded ./data/train/train.csv, 5 files out of an estimated total of 5\nUploaded 5 files\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "{\n  \"source\": [\n    \"('workspaceblobstore', 'cancer_data/cancer_data.csv')\"\n  ],\n  \"definition\": [\n    \"GetDatastoreFiles\",\n    \"ParseDelimited\",\n    \"DropColumns\",\n    \"SetColumnTypes\"\n  ],\n  \"registration\": {\n    \"id\": \"9873494e-db49-4adc-a5ed-70ff5deefffe\",\n    \"name\": \"raw_data\",\n    \"version\": 1,\n    \"workspace\": \"Workspace.create(name='cselscdhazureml', subscription_id='320d8d57-c87c-4434-827f-59ee7d86687a', resource_group='csels-cdh-dev')\"\n  }\n}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1661865915119
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_raw = ds_raw.as_named_input('raw_data')\r\n",
        "\r\n",
        "\r\n",
        "train_data = PipelineData(\"train_cancer_data\",datastore=data_store).as_dataset()\r\n",
        "test_data = PipelineData(\"test_cancer_data\",datastore=data_store).as_dataset()\r\n",
        "model_file = PipelineData(\"model_file\",datastore=data_store)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1661865915254
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data._input_mode)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "mount\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1661865915395
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a RunConfiguration to specify some additional requirements for this step.\r\n",
        "from azureml.core.runconfig import RunConfiguration,DockerConfiguration\r\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\r\n",
        "from azureml.core.conda_dependencies import CondaDependencies\r\n",
        "\r\n",
        "\r\n",
        "# create a new runconfig object\r\n",
        "run_config = RunConfiguration()\r\n",
        "\r\n",
        "# set Docker base image to the default CPU-based image\r\n",
        "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\r\n",
        "\r\n",
        "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\r\n",
        "run_config.environment.python.user_managed_dependencies = False\r\n",
        "\r\n",
        "# specify CondaDependencies obj\r\n",
        "run_config.environment.python.conda_dependencies = CondaDependencies.create(\r\n",
        "    conda_packages=['scikit-learn','pandas','numpy'],\r\n",
        "    pip_packages=['joblib','azureml-sdk'],\r\n",
        "    pin_sdk_version=False)\r\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1661865915530
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./scripts/prepare.py\r\n",
        "\r\n",
        "import argparse\r\n",
        "import os\r\n",
        "import sklearn\r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "from azureml.core import  Workspace\r\n",
        "from sklearn.metrics import f1_score,accuracy_score\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from azureml.core import Run, Dataset\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "def main():\r\n",
        "\r\n",
        "    parser = argparse.ArgumentParser(\"prepare\")\r\n",
        "\r\n",
        "    parser.add_argument(\"--input_data\",type=str)\r\n",
        "    parser.add_argument(\"--train\",type=str)\r\n",
        "    parser.add_argument(\"--test\",type=str)\r\n",
        "\r\n",
        "    args = parser.parse_args()\r\n",
        "\r\n",
        "    print(\"train args:\",args.train)\r\n",
        "    \r\n",
        "    run = Run.get_context()\r\n",
        "    ws = run.experiment.workspace\r\n",
        "    ds_tr = ws.get_default_datastore()\r\n",
        "\r\n",
        "\r\n",
        "    df = run.input_datasets['raw_data'].to_pandas_dataframe()\r\n",
        "\r\n",
        "    y = df['diagnosis'].astype('category')\r\n",
        "    X = df.drop('diagnosis',axis=1)\r\n",
        "\r\n",
        "    lbl_encoder = LabelEncoder()\r\n",
        "    y_encode = lbl_encoder.fit_transform(y)\r\n",
        "\r\n",
        "    print(\"cols:\",X.columns)\r\n",
        "    print(\"X shape\", X.shape)\r\n",
        "    print(\"encoder:\", lbl_encoder.classes_)\r\n",
        "    print(\"y encode:\", y_encode.shape)\r\n",
        "\r\n",
        "    x_train,x_test,y_train,y_test = train_test_split(X,y_encode,train_size=0.75,random_state=42,stratify =y_encode)\r\n",
        "\r\n",
        "    print(x_train.shape)\r\n",
        "    print(y_train.shape)\r\n",
        "\r\n",
        "    print(x_test.shape)\r\n",
        "    print(y_test.shape)\r\n",
        "\r\n",
        "    train = np.column_stack([x_train,y_train])\r\n",
        "    test = np.column_stack([x_test,y_test])\r\n",
        "   \r\n",
        "    # Write the model to file.\r\n",
        "    train_path = \"./data/train/\"\r\n",
        "    test_path = \"./data/test/\"\r\n",
        "\r\n",
        "    os.makedirs(args.train, exist_ok=True)\r\n",
        "    os.makedirs(args.test, exist_ok=True)\r\n",
        "    print(\"Saving the split\")\r\n",
        "\r\n",
        "    np.savetxt(os.path.join(args.train,\"train.csv\"), train, delimiter=\",\")\r\n",
        "    np.savetxt(os.path.join(args.test,\"test.csv\"), train, delimiter=\",\")\r\n",
        "  \r\n",
        "\r\n",
        " \r\n",
        "if __name__ =='__main__':\r\n",
        "    main()\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./scripts/prepare.py\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1661456432345
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./scripts/train2.py\r\n",
        "\r\n",
        "\r\n",
        "import argparse\r\n",
        "import os\r\n",
        "import sklearn\r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "from sklearn.metrics import f1_score,accuracy_score\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from azureml.core import Run, Dataset\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "import joblib\r\n",
        "\r\n",
        "def main():\r\n",
        "    parser = argparse.ArgumentParser(\"train\")\r\n",
        "    \r\n",
        "    parser.add_argument(\"--train\", type=str, help=\"train data\")\r\n",
        "    parser.add_argument(\"--test\", type=str, help=\"test data\")\r\n",
        "    parser.add_argument(\"--model_file\", type=str, help=\"model file\")\r\n",
        "    \r\n",
        "    args = parser.parse_args()\r\n",
        "    \r\n",
        "    run = Run.get_context()\r\n",
        "    ws = run.experiment.workspace\r\n",
        "    ds_tr = ws.get_default_datastore()\r\n",
        "\r\n",
        "    print(args.train)\r\n",
        "    print(args.test)\r\n",
        "\r\n",
        "    train = pd.read_csv(args.train+\"/train.csv\")\r\n",
        "    test = pd.read_csv(args.test+\"/test.csv\")\r\n",
        "\r\n",
        "    y_train = train.iloc[:,-1]\r\n",
        "    train.drop(columns = train.columns[-1],axis=1,inplace=True)\r\n",
        "    x_train = train\r\n",
        "\r\n",
        "    y_test = test.iloc[:,-1]\r\n",
        "    test.drop(columns = test.columns[-1],axis=1,inplace=True)\r\n",
        "    x_test = test\r\n",
        "\r\n",
        "    lbl_encoder = LabelEncoder()\r\n",
        "    y_encode = lbl_encoder.fit_transform(y_train)\r\n",
        "\r\n",
        "    print(\"cols:\",x_train.columns)\r\n",
        "    print(\"X shape\", x_train.shape)\r\n",
        "    print(\"encoder:\", lbl_encoder.classes_)\r\n",
        "    print(\"y encode:\", y_encode.shape)\r\n",
        "\r\n",
        "\r\n",
        "    print(x_train.shape)\r\n",
        "    print(y_train.shape)\r\n",
        "\r\n",
        "    print(x_test.shape)\r\n",
        "    print(y_test.shape)\r\n",
        "\r\n",
        "    rf = RandomForestClassifier(n_estimators=40,max_depth=100,max_features='auto',min_samples_leaf=3)\r\n",
        "    rf.fit(x_train,y_train)\r\n",
        "\r\n",
        "    accuracy = accuracy_score(y_test,rf.predict(x_test))\r\n",
        "    run.log(\"accuracy\",accuracy)\r\n",
        "\r\n",
        "    f1 = f1_score(y_test,rf.predict(x_test))\r\n",
        "    run.log(\"f1_score\",f1)\r\n",
        "\r\n",
        "\r\n",
        "    # Write the model to file.\r\n",
        "    # model_path = \"./outputs/cancer_model.pkl\"\r\n",
        "    os.makedirs(args.model_file, exist_ok=True)\r\n",
        "    joblib.dump(rf, args.model_file+\"/cancer_model.pkl\")\r\n",
        "\r\n",
        "    print('Saving the model to {}'.format(args.model_file+\"/cancer_model.pkl\"))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    run.complete()\r\n",
        "    \r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    main()\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./scripts/train2.py\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_directory ='./scripts'\r\n",
        "step1 = PythonScriptStep(name=\"prepare_step\",\r\n",
        "                         script_name=\"prepare.py\", \r\n",
        "                         arguments=[\"--input_data\",ds_raw,\"--train\",train_data,\"--test\",test_data],\r\n",
        "                         inputs=[ds_raw],\r\n",
        "                         outputs=[train_data,test_data],\r\n",
        "                         compute_target=aml_compute, \r\n",
        "                         runconfig=run_config,\r\n",
        "                         source_directory=source_directory,\r\n",
        "                         allow_reuse=True)\r\n",
        "print(\"Step1 created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step1 created\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1661865915935
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step2 = PythonScriptStep(name=\"train_step\",\r\n",
        "                         script_name=\"train2.py\", \r\n",
        "                         arguments=[\"--train\",train_data,\"--test\",test_data,\"--model_file\",model_file],\r\n",
        "                         inputs=[train_data,test_data],\r\n",
        "                         outputs=[model_file],\r\n",
        "                         compute_target=aml_compute, \r\n",
        "                         runconfig=run_config,\r\n",
        "                         source_directory=source_directory,\r\n",
        "                         allow_reuse=True)\r\n",
        "print(\"Step2 created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step2 created\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1661865916071
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./scripts/register.py\r\n",
        "import argparse\r\n",
        "from azureml.core import Run, Dataset\r\n",
        "from azureml.core.model import Model as AMLModel\r\n",
        "from azureml.core.resource_configuration import ResourceConfiguration\r\n",
        "\r\n",
        "def main():\r\n",
        "\r\n",
        "\r\n",
        "    parser = argparse.ArgumentParser(\"register\")\r\n",
        "    \r\n",
        "    parser.add_argument(\"--model_file\", type=str, help=\"model file\")\r\n",
        "    \r\n",
        "    args = parser.parse_args()\r\n",
        "    \r\n",
        "    run = Run.get_context()\r\n",
        "    ws = run.experiment.workspace\r\n",
        "    ds_tr = ws.get_default_datastore()\r\n",
        "\r\n",
        "    model_path = args.model_file+\"/cancer_model.pkl\"\r\n",
        "\r\n",
        "    print(\"model path:\",model_path)\r\n",
        "\r\n",
        "    AMLModel.register(workspace=ws,\r\n",
        "                      model_name=\"breast-cancer\",\r\n",
        "                      model_path=model_path,\r\n",
        "                      model_framework=AMLModel.Framework.SCIKITLEARN,\r\n",
        "                      model_framework_version=sklearn.__version__,\r\n",
        "                      resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\r\n",
        "                      description='Random forest classification model to predict breast cancer',\r\n",
        "                       tags={'area': 'diabetes', 'type': 'regression'})\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    main()\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./scripts/register.py\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step3 = PythonScriptStep(name=\"register_step\",\r\n",
        "                         script_name=\"register.py\", \r\n",
        "                         arguments=[\"--model_file\",model_file],\r\n",
        "                         inputs=[model_file],\r\n",
        "                         compute_target=aml_compute, \r\n",
        "                         runconfig=run_config,\r\n",
        "                         source_directory=source_directory,\r\n",
        "                         allow_reuse=True)\r\n",
        "print(\"Step3 created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step3 created\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1661866103905
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steps = [step1,step2,step3]\r\n",
        "pipeline1 = Pipeline(workspace=ws,steps=steps)\r\n",
        "run_exp = Experiment(workspace=ws, name=\"RF-BreastCancer-Pipeline\")"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1661866105811
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_exp.submit(pipeline1,regenerate_ouputs=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created step prepare_step [e4a9f44d][e3d441a6-23b2-4cfe-ba79-532aa41adbd9], (This step will run and generate new outputs)\nCreated step train_step [a0aa85d4][1abe2103-e0cf-4d14-b3c9-98140812eac1], (This step will run and generate new outputs)Created step register_step [9f0fb639][2373b19b-6082-4eda-85a0-ebabb7dc5d33], (This step will run and generate new outputs)\n\nSubmitted PipelineRun c4c66f66-4fa8-4b58-acc4-5df2ddf84317\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/c4c66f66-4fa8-4b58-acc4-5df2ddf84317?wsid=/subscriptions/320d8d57-c87c-4434-827f-59ee7d86687a/resourcegroups/csels-cdh-dev/workspaces/cselscdhazureml&tid=9ce70869-60db-44fd-abe8-d2767077fc8f\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "Run(Experiment: RF-BreastCancer-Pipeline,\nId: c4c66f66-4fa8-4b58-acc4-5df2ddf84317,\nType: azureml.PipelineRun,\nStatus: Preparing)",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>RF-BreastCancer-Pipeline</td><td>c4c66f66-4fa8-4b58-acc4-5df2ddf84317</td><td>azureml.PipelineRun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/c4c66f66-4fa8-4b58-acc4-5df2ddf84317?wsid=/subscriptions/320d8d57-c87c-4434-827f-59ee7d86687a/resourcegroups/csels-cdh-dev/workspaces/cselscdhazureml&amp;tid=9ce70869-60db-44fd-abe8-d2767077fc8f\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1661866112143
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}